{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Valent0296/allvent.github.io/blob/master/Test_3_tesi_AlexNet_PE2D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1bu_WTjGgq5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88e25aec-4c87-41d6-bf5a-aed34b6d77c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import sys\n",
        "sys.path.append('/MyDrive/math_loader/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNpVkeZy79d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e2782a3-437a-42b2-a533-e058c65df7f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n",
            "100%|██████████| 233M/233M [00:00<00:00, 295MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Conv2d(1, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
            "  (1): ReLU(inplace=True)\n",
            "  (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "  (4): ReLU(inplace=True)\n",
            "  (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (7): ReLU(inplace=True)\n",
            "  (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (9): ReLU(inplace=True)\n",
            "  (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (11): ReLU(inplace=True)\n",
            "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (13): AdaptiveAvgPool2d(output_size=(6, 6))\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "from torch.nn import Transformer\n",
        "from torchvision import datasets, transforms, models\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "from torch.utils.data import DataLoader\n",
        "import math\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#####SALVARE IL MODELLO UNA VOLTA ALLENATO, DOPODICHè IN UN ALTRO NOTEBOOK ANDARE AD ANALIZZARE PASSO PER PASSO COSA FA\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_ft = models.alexnet(pretrained=True).features.to(device)\n",
        "model_ft.add_module(\"0\", nn.Conv2d(1, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2)))\n",
        "model_ft.add_module(\"12\", nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1,1)))\n",
        "model_ft.add_module(\"13\", nn.AdaptiveAvgPool2d(output_size=(6,6)))\n",
        "print(model_ft)\n",
        "for name_ft in model_ft.modules():\n",
        "  name_ft.requires_grad_(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfSobE_EH0BY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d4f1e7e-a55b-401d-d11f-f87e1af48fa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/math_dataloader\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/math_dataloader/\n",
        "import data as d\n",
        "import language as l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-xqayIz3N2A"
      },
      "outputs": [],
      "source": [
        "filename = [\"data/\" + fi for fi in [\"train.txt\", \"test.txt\", \"validation.txt\"]]\n",
        "\n",
        "training = True\n",
        "height = 160\n",
        "width = 1600\n",
        "dataset=[]\n",
        "for i in range(3):\n",
        "  training = True if i == 1 else False\n",
        "  dataset.append(d.ExprDataset(\"data/\", filename[i], training, height,\n",
        "                          width, geometric_var=0.05,\n",
        "                          photometric_var=0.25))\n",
        "n_tok = len(dataset[0].tokens)\n",
        "\n",
        "train_dataloader = DataLoader(dataset[0], batch_size=64, shuffle=True)\n",
        "test_dataloader = DataLoader(dataset[1], batch_size=64, shuffle=True)\n",
        "val_dataloader = DataLoader(dataset[2], batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYH4tKyo967b"
      },
      "outputs": [],
      "source": [
        "#######################\n",
        "# Positional Encoding #\n",
        "#######################\n",
        "#1st version trial, future implementations will try diverse function\n",
        "class PositionalEncoding( nn.Module ):\n",
        "\n",
        "  def __init__( self, d_model: int=256, dropout: float=0.1, maxlen: int=10 ):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    den = torch.exp( torch.arange( 0, d_model, 2 ) * ( -math.log(10000) / d_model ) )\n",
        "    pos = torch.arange(0, maxlen).reshape( maxlen, 1 )\n",
        "    self.pe = torch.zeros( (maxlen, d_model) ).to(device)\n",
        "    self.pe[:, 0::2] = torch.sin(pos*den)\n",
        "    self.pe[:, 1::2] = torch.cos(pos*den)\n",
        "    self.pe.unsqueeze_(-2) #BE CAREFUL TO THIS ONE SINCE IT CHANGES DIMENSION\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.register_buffer(\"pos_enc\", self.pe)\n",
        "\n",
        "  def forward(self, ccn_embed: Tensor):\n",
        "    return self.dropout( ccn_embed + self.pe[ :ccn_embed.size(1), 0,: ] ) #We encode the first tok_embed.size(0) positions\n",
        "\n",
        "class PositionalEncoding2D( nn.Module ):\n",
        "\n",
        "  def __init__( self, d_model: int=256, dropout: float=0.1, height: int=10, width: int=10 ):\n",
        "    super(PositionalEncoding2D, self).__init__()\n",
        "    \"\"\"\n",
        "    :param d_model: dimension of the model\n",
        "    :param height: height of the positions\n",
        "    :param width: width of the positions\n",
        "    :return: d_model*height*width position matrix\n",
        "    \"\"\"\n",
        "    if d_model % 4 != 0:\n",
        "        raise ValueError(\"Cannot use sin/cos positional encoding with \"\n",
        "                         \"odd dimension (got dim={:d})\".format(d_model))\n",
        "    self.pe = torch.zeros(d_model, height, width).to(device)\n",
        "    # Each dimension use half of d_model\n",
        "    d_model = int(d_model / 2)\n",
        "    div_term = torch.exp(torch.arange(0., d_model, 2) *\n",
        "                         -(math.log(10000.0) / d_model))\n",
        "    pos_w = torch.arange(0., width).unsqueeze(1)\n",
        "    pos_h = torch.arange(0., height).unsqueeze(1)\n",
        "    self.pe[0:d_model:2, :, :] = torch.sin(pos_w * div_term).transpose(0, 1).unsqueeze(1).repeat(1, height, 1)\n",
        "    self.pe[1:d_model:2, :, :] = torch.cos(pos_w * div_term).transpose(0, 1).unsqueeze(1).repeat(1, height, 1)\n",
        "    self.pe[d_model::2, :, :] = torch.sin(pos_h * div_term).transpose(0, 1).unsqueeze(2).repeat(1, 1, width)\n",
        "    self.pe[d_model + 1::2, :, :] = torch.cos(pos_h * div_term).transpose(0, 1).unsqueeze(2).repeat(1, 1, width)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.register_buffer(\"pos_enc\", self.pe)\n",
        "\n",
        "  def forward(self, ccn_embed: Tensor):\n",
        "    return self.dropout( ccn_embed + self.pe) #We encode the first tok_embed.size(0) positions\n",
        "\n",
        "\n",
        "###################\n",
        "# Input embedding #\n",
        "###################\n",
        "#1st approach: try passing a CCN output as an imput embedding, (ex: 400x180)\n",
        "class SampleEmbedding( nn.Module ):\n",
        "\n",
        "  def __init__(self, d_emb: int=256):\n",
        "      super(SampleEmbedding, self).__init__()\n",
        "      self.ccn = model_ft\n",
        "\n",
        "  def forward(self, blocks: Tensor):\n",
        "    return torch.utils.checkpoint.checkpoint_sequential(self.ccn, 5, blocks).to(device)\n",
        "\n",
        "##################\n",
        "# Token Embedder #\n",
        "##################\n",
        "class TokenEmbedding(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size: int, d_model: int):\n",
        "    super(TokenEmbedding, self).__init__()\n",
        "    self.embedder = nn.Embedding(vocab_size, d_model)\n",
        "    self.emb_size = d_model\n",
        "\n",
        "  def forward(self, tokens: Tensor):\n",
        "    return (self.embedder( tokens.long() ) * math.sqrt(self.emb_size)).to(device)\n",
        "\n",
        "###########\n",
        "# Masking #\n",
        "###########\n",
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.tril(torch.ones((sz, sz))) == 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "def create_pad_mask( matrix: torch.tensor, pad_token: int) -> torch.tensor:\n",
        "    # If matrix = [1,2,3,0,0,0] where pad_token=0, the result mask is\n",
        "    # [False, False, False, True, True, True]\n",
        "    return (matrix == pad_token)\n",
        "\n",
        "############\n",
        "# Accuracy #\n",
        "############\n",
        "def batch_accuracy_calculate( pred: Tensor, y: Tensor):\n",
        "\n",
        "  corr = 0\n",
        "  tot = 0\n",
        "  for p, label in zip(pred[:] ,y[:]):\n",
        "\n",
        "\n",
        "    for tok_p, tok_l in zip(p[:], label[:]):\n",
        "\n",
        "\n",
        "      if tok_p.item() != 3 and tok_l.item() != 3:\n",
        "        corr += (tok_p == tok_l).type(torch.int).sum().item()\n",
        "        tot += 1\n",
        "\n",
        "\n",
        "      else:\n",
        "        return [corr, tot]\n",
        "\n",
        "  return [corr, tot]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClZ_t2igvmhl"
      },
      "outputs": [],
      "source": [
        "##############\n",
        "# Tranformer #\n",
        "##############\n",
        "class TransformerModel(nn.Module):\n",
        "\n",
        "  #ntokens referes to the latex vocabulary\n",
        "  def __init__(self, ntokens: int, d_mod: int, d_hid: int ,n_head: int=8, dropout: float=0.5):\n",
        "      super().__init__()\n",
        "      self.model_type = \"Transformer\"\n",
        "      self.nhead = n_head\n",
        "      self.d_model = d_mod\n",
        "      self.src_embedding = SampleEmbedding(d_mod)\n",
        "      self.pos_encod = PositionalEncoding2D(d_mod, dropout, 6, 6)\n",
        "      self.tgt_encod = PositionalEncoding(d_mod, dropout, maxlen=20)\n",
        "      self.tgt_embed = nn.Embedding(n_tok,d_mod)\n",
        "      self.flat = nn.Flatten(start_dim=2)\n",
        "      encoder_layer = nn.TransformerEncoderLayer( d_mod, n_head, d_hid, batch_first=True )\n",
        "      self.transformer_encoder = nn.TransformerEncoder(encoder_layer, n_head)\n",
        "      decoder_layer = nn.TransformerDecoderLayer( d_mod, n_head, dropout=dropout, batch_first=True)\n",
        "      self.transformer_decoder = nn.TransformerDecoder(decoder_layer, n_head )\n",
        "      self.lin_soft_stack = nn.Sequential(\n",
        "          nn.Linear(d_mod, ntokens),\n",
        "          nn.Softmax(dim=1)\n",
        "      )\n",
        "      #self.init_weights()\n",
        "\n",
        "  \"\"\"def init_weights(self) -> None:\n",
        "    initrange = 0.1\n",
        "    self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "    self.lin.bias.data.zero_()\n",
        "    self.lin.weight.data.uniform_(-initrange, initrange)\"\"\"\n",
        "\n",
        "  def forward(self, src: Tensor, tgt: Tensor, tgt_mask: Tensor, pad_mask: Tensor) -> Tensor:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "       src: Tensor, shape [seq_len, batch_size]\n",
        "\n",
        "    Returns:\n",
        "       output Tensor of shape [seq_len, batch_size, ntoken]\n",
        "    \"\"\"\n",
        "    src_emb = self.src_embedding(src) * math.sqrt(self.d_model)\n",
        "\n",
        "    tgt = self.tgt_embed(tgt) * math.sqrt(self.d_model)\n",
        "    src_emb = self.pos_encod(src_emb)\n",
        "    tgt = self.tgt_encod(tgt)\n",
        "    src_emb = self.flat(src_emb)\n",
        "    #print(src_emb.size())\n",
        "    memory = self.transformer_encoder(src_emb.permute(0,2,1))\n",
        "    deco = self.transformer_decoder(tgt, memory, tgt_mask=tgt_mask, tgt_key_padding_mask=pad_mask)\n",
        "    output = self.lin_soft_stack(deco)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOmh8Ntq-SNz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "9ce51dab-16d7-4edf-eb92-928987714215"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'for batch, (X, y, paths) in enumerate(test_dataloader):\\n    X, y = X.to(device), y.to(device)\\n    pred = model(X)\\n\\nprint(pred.size())'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "model = TransformerModel(n_tok, 256,1024).to(device)\n",
        "#model = SampleEmbedding(512)\n",
        "model = torch.nn.DataParallel(model)\n",
        "\"\"\"for batch, (X, y, paths) in enumerate(test_dataloader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    pred = model(X)\n",
        "\n",
        "print(pred.size())\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48akcIKuCSC_"
      },
      "outputs": [],
      "source": [
        "#Define train and test loop\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "  model.train()\n",
        "  size = 8300*20\n",
        "  for batch, (X, y, paths) in enumerate(dataloader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    tgt_mask = generate_square_subsequent_mask(y.size(1))\n",
        "    pad_mask = create_pad_mask(y, 3)\n",
        "    #Compute prediction and loss\n",
        "    pred = model(X, y, tgt_mask, pad_mask)\n",
        "    #print(pred.size())\n",
        "    loss = loss_fn(pred.permute(0,2,1), y)\n",
        "\n",
        "    #Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "   # print(\"Sono qui \", batch)\n",
        "    loss.backward()\n",
        "    #print(\"E ora qui\", batch)\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch % 10 == 0:\n",
        "      loss, current = loss.item(), batch * len(X)\n",
        "      print(f\"Funzione di loss: {loss:>7f} [{current:>5d}/8300]\")\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    size = 0\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for X, y, paths in dataloader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        tgt_mask = generate_square_subsequent_mask(y.size(1))\n",
        "        pad_mask = create_pad_mask(y, 3)\n",
        "        #Compute prediction and loss\n",
        "        pred = model(X, y, tgt_mask, pad_mask)\n",
        "        loss = loss_fn(pred.permute(0,2,1), y)\n",
        "        test_loss += loss_fn(pred.permute(0,2,1), y).item()\n",
        "        acc_pack = batch_accuracy_calculate(pred.argmax(2), y) #(pred.argmax(2) == y).type(torch.int).sum().item()\n",
        "        correct += acc_pack[0]\n",
        "        size += acc_pack[1]\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Errore test: \\n Accuratezza: {(100*correct):>0.1f}%, Media loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyzO1CzHGMve",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c994e440-ee25-45d8-c3cc-db6562e7d826"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Funzione di loss: 4.606185 [    0/8300]\n",
            "Funzione di loss: 4.603224 [  640/8300]\n",
            "Funzione di loss: 4.603198 [ 1280/8300]\n",
            "Funzione di loss: 4.599563 [ 1920/8300]\n",
            "Funzione di loss: 4.584363 [ 2560/8300]\n",
            "Funzione di loss: 4.548936 [ 3200/8300]\n",
            "Funzione di loss: 4.540801 [ 3840/8300]\n",
            "Funzione di loss: 4.527478 [ 4480/8300]\n",
            "Funzione di loss: 4.509054 [ 5120/8300]\n",
            "Funzione di loss: 4.458724 [ 5760/8300]\n",
            "Funzione di loss: 4.405024 [ 6400/8300]\n",
            "Funzione di loss: 4.391484 [ 7040/8300]\n",
            "Funzione di loss: 4.351495 [ 7680/8300]\n",
            "Errore test: \n",
            " Accuratezza: 56.5%, Media loss: 4.284578 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Funzione di loss: 4.303141 [    0/8300]\n",
            "Funzione di loss: 4.319856 [  640/8300]\n",
            "Funzione di loss: 4.272949 [ 1280/8300]\n",
            "Funzione di loss: 4.252143 [ 1920/8300]\n",
            "Funzione di loss: 4.243358 [ 2560/8300]\n",
            "Funzione di loss: 4.186568 [ 3200/8300]\n",
            "Funzione di loss: 4.191045 [ 3840/8300]\n",
            "Funzione di loss: 4.127498 [ 4480/8300]\n",
            "Funzione di loss: 4.090259 [ 5120/8300]\n",
            "Funzione di loss: 4.124470 [ 5760/8300]\n",
            "Funzione di loss: 4.057249 [ 6400/8300]\n",
            "Funzione di loss: 4.110947 [ 7040/8300]\n",
            "Funzione di loss: 4.055350 [ 7680/8300]\n",
            "Errore test: \n",
            " Accuratezza: 79.8%, Media loss: 4.042353 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Funzione di loss: 4.059374 [    0/8300]\n",
            "Funzione di loss: 4.080653 [  640/8300]\n",
            "Funzione di loss: 4.080417 [ 1280/8300]\n",
            "Funzione di loss: 4.068047 [ 1920/8300]\n",
            "Funzione di loss: 4.060451 [ 2560/8300]\n",
            "Funzione di loss: 4.062108 [ 3200/8300]\n",
            "Funzione di loss: 4.060243 [ 3840/8300]\n",
            "Funzione di loss: 4.026669 [ 4480/8300]\n",
            "Funzione di loss: 4.059474 [ 5120/8300]\n",
            "Funzione di loss: 4.068377 [ 5760/8300]\n",
            "Funzione di loss: 4.056062 [ 6400/8300]\n",
            "Funzione di loss: 4.021814 [ 7040/8300]\n",
            "Funzione di loss: 4.020685 [ 7680/8300]\n",
            "Errore test: \n",
            " Accuratezza: 94.5%, Media loss: 3.984750 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Funzione di loss: 4.033921 [    0/8300]\n",
            "Funzione di loss: 4.031286 [  640/8300]\n",
            "Funzione di loss: 4.024891 [ 1280/8300]\n",
            "Funzione di loss: 4.011243 [ 1920/8300]\n",
            "Funzione di loss: 4.008015 [ 2560/8300]\n",
            "Funzione di loss: 3.993975 [ 3200/8300]\n",
            "Funzione di loss: 4.020988 [ 3840/8300]\n",
            "Funzione di loss: 4.008248 [ 4480/8300]\n",
            "Funzione di loss: 3.996783 [ 5120/8300]\n",
            "Funzione di loss: 3.992932 [ 5760/8300]\n",
            "Funzione di loss: 3.998923 [ 6400/8300]\n",
            "Funzione di loss: 4.007398 [ 7040/8300]\n",
            "Funzione di loss: 3.985363 [ 7680/8300]\n",
            "Errore test: \n",
            " Accuratezza: 91.6%, Media loss: 3.983404 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Funzione di loss: 4.013205 [    0/8300]\n",
            "Funzione di loss: 3.956005 [  640/8300]\n",
            "Funzione di loss: 4.003137 [ 1280/8300]\n",
            "Funzione di loss: 4.013011 [ 1920/8300]\n",
            "Funzione di loss: 4.001699 [ 2560/8300]\n",
            "Funzione di loss: 3.995059 [ 3200/8300]\n",
            "Funzione di loss: 4.007252 [ 3840/8300]\n",
            "Funzione di loss: 3.987876 [ 4480/8300]\n",
            "Funzione di loss: 3.972620 [ 5120/8300]\n",
            "Funzione di loss: 3.951379 [ 5760/8300]\n",
            "Funzione di loss: 3.997933 [ 6400/8300]\n",
            "Funzione di loss: 3.975006 [ 7040/8300]\n",
            "Funzione di loss: 3.971879 [ 7680/8300]\n",
            "Errore test: \n",
            " Accuratezza: 94.4%, Media loss: 3.986149 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "Funzione di loss: 4.007737 [    0/8300]\n",
            "Funzione di loss: 3.986983 [  640/8300]\n",
            "Funzione di loss: 3.959787 [ 1280/8300]\n",
            "Funzione di loss: 3.987461 [ 1920/8300]\n",
            "Funzione di loss: 3.944386 [ 2560/8300]\n",
            "Funzione di loss: 3.992479 [ 3200/8300]\n",
            "Funzione di loss: 3.979841 [ 3840/8300]\n",
            "Funzione di loss: 3.990937 [ 4480/8300]\n",
            "Funzione di loss: 3.948058 [ 5120/8300]\n",
            "Funzione di loss: 4.014686 [ 5760/8300]\n",
            "Funzione di loss: 4.003342 [ 6400/8300]\n",
            "Funzione di loss: 3.971096 [ 7040/8300]\n",
            "Funzione di loss: 3.999547 [ 7680/8300]\n",
            "Errore test: \n",
            " Accuratezza: 97.8%, Media loss: 3.982339 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "Funzione di loss: 4.000997 [    0/8300]\n",
            "Funzione di loss: 3.994501 [  640/8300]\n",
            "Funzione di loss: 3.986798 [ 1280/8300]\n",
            "Funzione di loss: 3.992731 [ 1920/8300]\n",
            "Funzione di loss: 3.962929 [ 2560/8300]\n",
            "Funzione di loss: 3.988420 [ 3200/8300]\n",
            "Funzione di loss: 3.997990 [ 3840/8300]\n",
            "Funzione di loss: 3.997953 [ 4480/8300]\n",
            "Funzione di loss: 3.965900 [ 5120/8300]\n",
            "Funzione di loss: 3.981764 [ 5760/8300]\n",
            "Funzione di loss: 3.996897 [ 6400/8300]\n",
            "Funzione di loss: 3.983515 [ 7040/8300]\n",
            "Funzione di loss: 3.993333 [ 7680/8300]\n",
            "Errore test: \n",
            " Accuratezza: 97.7%, Media loss: 3.982386 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "Funzione di loss: 3.993211 [    0/8300]\n",
            "Funzione di loss: 3.972068 [  640/8300]\n",
            "Funzione di loss: 4.011170 [ 1280/8300]\n",
            "Funzione di loss: 3.984834 [ 1920/8300]\n",
            "Funzione di loss: 3.982534 [ 2560/8300]\n",
            "Funzione di loss: 3.971743 [ 3200/8300]\n",
            "Funzione di loss: 3.963614 [ 3840/8300]\n",
            "Funzione di loss: 3.988729 [ 4480/8300]\n",
            "Funzione di loss: 3.988408 [ 5120/8300]\n",
            "Funzione di loss: 3.955798 [ 5760/8300]\n",
            "Funzione di loss: 4.017283 [ 6400/8300]\n",
            "Funzione di loss: 3.958813 [ 7040/8300]\n",
            "Funzione di loss: 3.984656 [ 7680/8300]\n",
            "Errore test: \n",
            " Accuratezza: 97.2%, Media loss: 3.972754 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "Funzione di loss: 3.989503 [    0/8300]\n",
            "Funzione di loss: 3.965353 [  640/8300]\n",
            "Funzione di loss: 4.020982 [ 1280/8300]\n",
            "Funzione di loss: 3.972382 [ 1920/8300]\n",
            "Funzione di loss: 4.002891 [ 2560/8300]\n",
            "Funzione di loss: 3.981751 [ 3200/8300]\n",
            "Funzione di loss: 3.991044 [ 3840/8300]\n",
            "Funzione di loss: 3.995224 [ 4480/8300]\n",
            "Funzione di loss: 3.966650 [ 5120/8300]\n",
            "Funzione di loss: 3.972036 [ 5760/8300]\n",
            "Funzione di loss: 3.969934 [ 6400/8300]\n",
            "Funzione di loss: 4.010831 [ 7040/8300]\n",
            "Funzione di loss: 3.982367 [ 7680/8300]\n",
            "Errore test: \n",
            " Accuratezza: 97.3%, Media loss: 3.971722 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "Funzione di loss: 3.961894 [    0/8300]\n",
            "Funzione di loss: 3.986678 [  640/8300]\n",
            "Funzione di loss: 3.958045 [ 1280/8300]\n",
            "Funzione di loss: 3.998688 [ 1920/8300]\n",
            "Funzione di loss: 3.989442 [ 2560/8300]\n",
            "Funzione di loss: 3.970359 [ 3200/8300]\n",
            "Funzione di loss: 3.988057 [ 3840/8300]\n",
            "Funzione di loss: 3.978098 [ 4480/8300]\n",
            "Funzione di loss: 3.963691 [ 5120/8300]\n",
            "Funzione di loss: 3.989707 [ 5760/8300]\n",
            "Funzione di loss: 3.977165 [ 6400/8300]\n",
            "Funzione di loss: 4.007567 [ 7040/8300]\n",
            "Funzione di loss: 3.976662 [ 7680/8300]\n",
            "Errore test: \n",
            " Accuratezza: 93.7%, Media loss: 3.982472 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "#Definiamo la loss function e il metodo di minimizzazione\n",
        "learning_rate = 1e-1\n",
        "momentum = 0.9\n",
        "batch_size = 64\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum = momentum)\n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.98), eps=1e-9)\n",
        "#train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
        "#test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "#Definire i tgt ovvero gli output\n",
        "\n",
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")\n",
        "\n",
        "torch.save(model.state_dict(), 'model_weights.pth')\n",
        "torch.save(model, 'model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'model_weights.pth')\n",
        "torch.save(model, 'model.pth')"
      ],
      "metadata": {
        "id": "F8G2Oq2mlHHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%debug"
      ],
      "metadata": {
        "id": "Cbq2HQln3bx-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f66cfbc-5f04-4a42-b191-595ea6532c48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:No traceback has been produced, nothing to debug.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYcLMxx5gQwxGzn+7xYmr0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}